#!/usr/bin/env python
# coding: utf-8

# In[5]:


# import libraries
import sklearn
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
#from sklearn.linear_model import LogisticRegression
#from azureml.explain.model.tabular_explainer import TabularExplainer
from sklearn_pandas import DataFrameMapper
import pandas as pd
import numpy as np
import folium
import json
import urllib
import urllib.request
from urllib.request import urlopen
import tabula # to scrape table data from PDF
import geojson
import matplotlib.pyplot as plt
import seaborn as sns
import requests # library to handle requests
from scipy.spatial.distance import cdist
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn import cluster
print("libraries imported again")


# In[23]:


# read the 
import pandas as pd
import numpy as np

datafile= 'G:\\BA资料\\DM\\GA\\fullmdr1.csv'  # 第一张表属性标签
resultfile = 'G:\\BA资料\\DM\\GA\\Clustering_Full.csv'  # 数据输出路径

# 读取原始数据，指定UTF-8编码（需要用文本编辑器将数据装换为UTF-8编码）
df_Madrid_Districts_GPS = pd.read_csv(datafile, encoding = 'utf-8')


df_Madrid_Districts_GPS.head()


# In[24]:


index3 = df_Madrid_Districts_GPS['Rental Yield']>= 0

#删除2.29日数据

df_Madrid_Districts_GPS_clean = df_Madrid_Districts_GPS[ index3 ]
print('数据清洗后数据的形状为：',df_Madrid_Districts_GPS_clean.shape)
cleanedfile = 'G:\\BA资料\\DM\\GA\\Cleaned_MDRGPS.csv'  # 数据输出路径

df_Madrid_Districts_GPS_clean.to_csv(cleanedfile)  # 保存清洗后的数据
df_Madrid_Districts_GPS=df_Madrid_Districts_GPS_clean[['District','lat','long','Rental Yield']]
df_Madrid_Districts_GPS


# In[25]:


# Create labels for rent price, rooms...
Rental_Yield_labels   = range(1, 50)
len(Rental_Yield_labels)


# In[26]:


Rental_Yield_groups = pd.qcut(df_Madrid_Districts_GPS['Rental Yield'], q=49, labels = Rental_Yield_labels, duplicates='drop')


# In[27]:



# Create new columns: rent_price_groups,sq_mt_built_groups,n_rooms_groups,floor_groups
Housing_style = df_Madrid_Districts_GPS.assign(Rental_Yield_groups = Rental_Yield_groups.values)

# Create "pollution scoring": the lower the score, the less polluted the area is
# def concatenate_scores(x): return str(x['NO2_groups']) + str(x['PM10_groups']) + str(x['OZONE_groups'])
def concatenate_scores(x): return x['Rental_Yield_groups']

Housing_style['Housing_scoring'] = Housing_style.apply(concatenate_scores, axis=1)

final_Housing_score = Housing_style.sort_values(by='Housing_scoring')

final_Housing_score['Housing_scoring'] = final_Housing_score['Housing_scoring']/4 # to average pollution scoring
final_Housing_score


# In[28]:


result_file = pd.DataFrame(final_Housing_score[1:])
result_file.to_csv(resultfile)  # 导出结果


# In[29]:


MADRID_MAP_HEATMAP = folium.Map(location=[40.4165001, -3.7025599], zoom_start=9,tiles='cartodbpositron') # Madr

tiles = ['stamenwatercolor', 'cartodbpositron', 'openstreetmap', 'stamenterrain']
for tile in tiles:    folium.TileLayer(tile).add_to(MADRID_MAP_HEATMAP)


# In[30]:


geo_path = 'G:\\BA资料\\DM\\GA\\distritos.geojson'


# In[31]:


#create empty lists used for storing their final scores


City1,City2,City3,City4,City5,City6,City7,City8,City9,City10,City11,City12,City13,City14,City15,City16,City17,City18,City19,City20=([] for i in range(20))


# In[39]:


def Avg(list):
    Add_row=[]
    s=0
    score=0
    for i in list:
        s=s+i[3]
    score=s/len(list)
    return score


# In[40]:


c=0

data_array = np.array(final_Housing_score)
data_list =data_array.tolist()
for data in data_list:
    if data[0] ==' Arganzuela':
        City1.append(data)  
    elif data[0]==' Barajas':
        City2.append(data)
    elif data[0]==' Carabanchel':
        City3.append(data)
    elif data[0]==' Centro':
        City4.append(data)
    elif data[0]==' Chamartín':
        City5.append(data)
    elif data[0]==' Chamberí':
        City6.append(data)
    elif data[0]==' Ciudad Lineal':
        City7.append(data)
    elif data[0]==' Fuencarral':
        City8.append(data)
    elif data[0]==' Hortaleza':
        City9.append(data)
    elif data[0]==' Moncloa':
        City10.append(data)
    elif data[0]==' Moratalaz':
        City11.append(data)
    elif data[0]==' Puente de Vallecas':
        City12.append(data)
    elif data[0]==' Retiro':
        City13.append(data)
    elif data[0]==' Salamanca':
        City14.append(data)
    elif data[0]==' Tetuán':
        City15.append(data)
    elif data[0]==' Usera':
        City16.append(data)
    elif data[0]==' Villa de Vallecas':
        City17.append(data)
    elif data[0]==' Vicálvaro':
        City18.append(data)
    elif data[0]==' Latina':
        City19.append(data)    
    elif data[0]==' Villaverde':
        City20.append(data)
            
score,s=0,0

for i in City1:
    s=s+i[3]
score1=s/len(City1)
list=[City1[0][0],score1]
list


# In[41]:


print(Avg(City1),'\n',
     Avg(City2),'\n',
     Avg(City3),'\n',
     Avg(City4),'\n',
      Avg(City5),'\n',
     Avg(City6),'\n',
     Avg(City7),'\n',
     Avg(City8),'\n',
     Avg(City9),'\n',
     Avg(City10),'\n',
     Avg(City11),'\n',
     Avg(City12),'\n',
     Avg(City13),'\n',
     Avg(City14),'\n',
     Avg(City15),'\n',
     Avg(City16),'\n',
     Avg(City17),'\n',
     Avg(City18),'\n',
     Avg(City19),'\n',
     Avg(City20),'\n')


# In[42]:


datafile= 'G:\\BA资料\\DM\\GA\\Districts_with_scores.csv'  # 第一张表属性标签

# Illustrate all the scores within this dataset
data = pd.read_csv(datafile, encoding = 'utf-8')
data


# In[43]:



choropleth = folium.Choropleth(geo_data=geo_path, 
                               data=data,  
                               columns=['District', 'Housing_score'], 
                               key_on='feature.properties.nombre',
                               fill_color='YlOrRd', 
                               fill_opacity=0.8,  
                               line_opacity=0.8, 
                               legend_name='Housing Scores of Madrid',).add_to(MADRID_MAP_HEATMAP)
choropleth.geojson.add_child(    folium.features.GeoJsonTooltip(['nombre'], labels=False,font=40))

MADRID_MAP_HEATMAP


# In[205]:


import xlrd
#打开excel
wb = xlrd.open_workbook('G:\\BA资料\\DM\\GA\\1.xlsx')
#按工作簿定位工作表
sh = wb.sheet_by_name('1')
print(sh.nrows)#有效数据行数
print(sh.ncols)#有效数据列数
print(sh.cell(0,0).value)#输出第一行第一列的值
print(sh.row_values(0))#输出第一行的所有值
#将数据和标题组合成字典
print(dict(zip(sh.row_values(0),sh.row_values(1))))
#遍历excel，打印所有数据
for i in range(sh.nrows):
    print(sh.row_values(i))


# In[53]:


print(sh.nrows_values)


# In[14]:


import pandas as pd

df = pd.read_excel('G:\\BA资料\\DM\\GA\\1.xlsx')
data=df.values
print("获取到所有的值:\n{}".format(data))


# In[ ]:


# create MADRID map

MADRID_MAP_HEATMAP.choropleth(
 geo_path ,
 data = data,
 columns = ['District', 'Housing_score'],
 key_on = 'feature.properties.nombre',
 fill_color = 'YlOrRd',
 fill_opacity = 0.6,
 line_opacity = 1,
 legend_name = 'Madrid: 2019 pollution scoring by district')

MADRID_MAP_HEATMAP.geojson.add_child(folium.features.GeoJsonTooltip(['nombre'], labels=True))
#Display Map
MADRID_MAP_HEATMAP
# higher scores show higher pollution districts

#!/usr/bin/env python
# coding: utf-8

# In[96]:


# 对数据进行基本的探索
# 返回缺失值个数以及最大最小值
import pandas as pd
import numpy as np
datafile= 'G:\\BA资料\\DM\\GA\\TFMadrid Housing.csv'  # 第一张表属性标签
datafile1= 'G:\\BA资料\\DM\\GA\\fullMadridHousing1.csv'  # 第一张表属性标签

resultfile = 'G:\\BA资料\\DM\\GA\\Result.csv'  # 数据输出路径
resultfile = 'G:\\BA资料\\DM\\GA\\Result1.csv'  # 数据输出路径
# 读取原始数据，指定UTF-8编码（需要用文本编辑器将数据装换为UTF-8编码）
data = pd.read_csv(datafile, encoding = 'utf-8')
data1 = pd.read_csv(datafile1, encoding = 'utf-8')
data.head()
data1.head()


# In[97]:


data.head()


# In[98]:


data.info()


# In[99]:


data1.info()


# In[100]:


import pandas as pd
import numpy as np
from pandas import DataFrame
from pandas import Series
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt# 客户信息类别
import pylab


# In[101]:


print('原始数据的形状为：',data1.shape)
# 去除票价为空的记录
# 只保留票价非零的，或者平均折扣率不为0且总飞行公里数大于0的记录。

index3 = data1['Rental Yield']>= 0
index2 = data1['bathroom'] != 0
index4 = data['ROI']>= 0
#删除2.29日数据

data_clean1 = data1[ index3&index2 ]
data_clean = data[ index4 ]

print('数据清洗后数据的形状为：',data_clean1.shape)
cleanedfile = 'G:\\BA资料\\DM\\GA\\Cleaned_FullMDR.csv'  # 数据输出路径
data_clean.to_csv(cleanedfile) 

data1=data_clean1[['sq_mt_built','n_rooms','bathroom','Rental Yield']]
data1


# In[102]:


data=data_clean[['heating','has_ac','has_lift','has_pool','has_storage_room','has_parking','ROI']]

cleanedfile = 'G:\\BA资料\\DM\\GA\\Cleaned_TFMDR.csv'  # 数据输出路径
data_clean.to_csv(cleanedfile) 

data.info()


# In[103]:


dataNew=data
dataNew1=data1


# In[104]:


def Conversion1(matrix):
    Add_row=[]
    for i in range(len(matrix)):
        if matrix[i]==1.0:
            K_B=1
        else:
            K_B=-1
        Add_row.append(K_B)
    Add_row= DataFrame(
                {"K_B":Add_row}
                )
    dataNew.loc[:,('K_B')]=Add_row

def Conversion2(matrix):
    Add_row=[]
    for i in range(len(matrix)):
        if matrix[i]==1.0:
            K_B=1
        elif matrix[i]==0:
            K_B=0
        else:
            K_B=-1
        Add_row.append(K_B)
    Add_row= DataFrame(
                {"K_B":Add_row}
                )
    dataNew.loc[:,('K_B')]=Add_row
    
def Conversion3(matrix):
    Add_row=[]
    for i in range(len(matrix)):
        if matrix[i]==1.0:
            K_B=1
        else:
            K_B=-1
        Add_row.append(K_B)
    Add_row= DataFrame(
                {"K_B":Add_row}
                )
    dataNew.loc[:,('K_B')]=Add_row


# In[105]:


dataNew = data


Conversion1(dataNew['has_ac'])
dataNew.rename(columns={'K_B':'has_ac_B'},inplace = True)

Conversion1(dataNew['has_pool'])
dataNew.rename(columns={'K_B':'has_pool_B'},inplace = True)



Conversion1(dataNew['has_storage_room'])
dataNew.rename(columns={'K_B':'has_storage_room_B'},inplace = True)

print('：\n',dataNew)


# In[106]:


dataNew


# In[107]:



dataNew.rename(columns={'has_ac':'ac','has_lift':'lift','has_pool':'pool',
                                  'has_storage_room':'SR','has_parking':'parking'},inplace = True)
dataNew_selection=dataNew[['ROI','parking','lift','SR','pool']]
print(dataNew_selection)


# In[108]:


data1_selection = data1[['Rental Yield','sq_mt_built',
                  'n_rooms','bathroom']]

print(data1_selection)


# In[109]:


from sklearn.preprocessing import StandardScaler
data_STD = StandardScaler().fit_transform(dataNew_selection)
np.savez('G:\\BA资料\\DM\\GA\\TFMDR_scale.npz',data_STD)
print('After Standardizing：\n',data_STD[:5,:])


# In[110]:


from sklearn.preprocessing import StandardScaler
data_STD1 = StandardScaler().fit_transform(data1_selection)
np.savez('G:\\BA资料\\DM\\GA\\FULLMDR_scale.npz',data_STD1)
print('After Standardizing：\n',data_STD1[:5,:])


# In[111]:


zscore_data = (data_STD - data_STD.mean(axis=0))/(data_STD.std(axis=0))
zscore_data[0:5]
def distEclud(vecA, vecB):
    """
    计算两个向量的欧式距离的平方，并返回
    """
    return np.sum(np.power(vecA - vecB, 2))
 
def test_Kmeans_nclusters(data_train):
    """
    计算不同的k值时，SSE的大小变化
    """
    #data_train = data_train.values
    nums=range(2,10)
    SSE = []
    for num in nums:
        sse = 0
        kmodel = KMeans(n_clusters=num, n_jobs=4)
        kmodel.fit(data_train)
        # 簇中心
        cluster_ceter_list = kmodel.cluster_centers_
        # 个样本属于的簇序号列表
        cluster_list = kmodel.labels_.tolist()
        for index in  range(len(data)):
            cluster_num = cluster_list[index]
            sse += distEclud(data_train[index, :], cluster_ceter_list[cluster_num])
        print("簇数是",num , "时； SSE是", sse)
        SSE.append(sse)
    return nums, SSE
 
nums, SSE = test_Kmeans_nclusters(zscore_data)


# In[112]:


zscore_data1 = (data_STD1 - data_STD1.mean(axis=0))/(data_STD1.std(axis=0))
zscore_data1[0:5]
def distEclud(vecA, vecB):
    """
    计算两个向量的欧式距离的平方，并返回
    """
    return np.sum(np.power(vecA - vecB, 2))
 
def test_Kmeans_nclusters1(data_train):
    """
    计算不同的k值时，SSE的大小变化
    """
    #data_train = data_train.values
    nums1=range(2,10)
    SSE1 = []
    for num in nums1:
        sse = 0
        kmodel = KMeans(n_clusters=num, n_jobs=4)
        kmodel.fit(data_train)
        # 簇中心
        cluster_ceter_list = kmodel.cluster_centers_
        # 个样本属于的簇序号列表
        cluster_list = kmodel.labels_.tolist()
        for index in  range(len(data1)):
            cluster_num = cluster_list[index]
            sse += distEclud(data_train[index, :], cluster_ceter_list[cluster_num])
        print("簇数是",num , "时； SSE是", sse)
        SSE1.append(sse)
    return nums1, SSE1
 
nums1, SSE1 = test_Kmeans_nclusters1(zscore_data1)


# In[113]:


#画图，通过观察SSE与k的取值尝试找出合适的k值
# 中文和负号的正常显示
plt.rcParams['font.sans-serif'] = 'SimHei'
plt.rcParams['font.size'] = 12.0
plt.rcParams['axes.unicode_minus'] = False
# 使用ggplot的绘图风格
plt.style.use('ggplot')
## 绘图观测SSE与簇个数的关系
fig=plt.figure(figsize=(10, 8))
ax=fig.add_subplot(1,1,1)
ax.plot(nums,SSE,marker="+")
ax.set_xlabel("n_clusters", fontsize=18)
ax.set_ylabel("SSE", fontsize=18)
fig.suptitle("KMeans", fontsize=20)
plt.savefig(fname="Kmeans.png",figsize=[10,10])
plt.show()


# In[114]:


#画图，通过观察SSE与k的取值尝试找出合适的k值
# 中文和负号的正常显示
plt.rcParams['font.sans-serif'] = 'SimHei'
plt.rcParams['font.size'] = 12.0
plt.rcParams['axes.unicode_minus'] = False
# 使用ggplot的绘图风格
plt.style.use('ggplot')
## 绘图观测SSE与簇个数的关系
fig=plt.figure(figsize=(10, 8))
ax1=fig.add_subplot(1,1,1)
ax1.plot(nums1,SSE1,marker="+")
ax1.set_xlabel("n_clusters", fontsize=18)
ax1.set_ylabel("SSE1", fontsize=18)
fig.suptitle("KMeans1", fontsize=20)
plt.savefig(fname="Kmeans1.png",figsize=[10,10])
plt.show()


# In[61]:


import pandas as pd
import numpy as np
from sklearn.cluster import KMeans  # 导入kmeans算法

# 读取标准化后的数据
BANK_scale = np.load('G:\\BA资料\\DM\\GA\\TFMDR_scale.npz')['arr_0']
k = 4  # 确定聚类中心数

# 构建模型，随机种子设为123
kmeans_model = KMeans(n_clusters=k,n_jobs=4,random_state=123)
fit_kmeans = kmeans_model.fit(BANK_scale)  # 模型训练

# 查看聚类结果
kmeans_cc = kmeans_model.cluster_centers_  # 聚类中心
print('各类聚类中心为：\n',kmeans_cc)
kmeans_labels = kmeans_model.labels_  # 样本的类别标签
print('各样本的类别标签为：\n',kmeans_labels)
r1 = pd.Series(kmeans_model.labels_).value_counts()  # 统计不同类别样本的数目
print('最终每个类别的数目为：\n',r1)
# 输出聚类分群的结果
cluster_center = pd.DataFrame(kmeans_model.cluster_centers_,             columns=['ROI',
                      'parking','lift','SR','pool'])   # 将聚类中心放在数据框中
cluster_center.index = pd.DataFrame(kmeans_model.labels_ ).                  drop_duplicates().iloc[:,0]  # 将样本类别作为数据框索引
print(cluster_center)


# In[115]:


import pandas as pd
import numpy as np
from sklearn.cluster import KMeans  # 导入kmeans算法

# 读取标准化后的数据
MDR1_scale = np.load('G:\\BA资料\\DM\\GA\\FULLMDR_scale.npz')['arr_0']
k = 5  # 确定聚类中心数

# 构建模型，随机种子设为123
kmeans_model1 = KMeans(n_clusters=k,n_jobs=4,random_state=123)
fit_kmeans1 = kmeans_model1.fit(MDR1_scale)  # 模型训练

# 查看聚类结果
kmeans_cc1 = kmeans_model1.cluster_centers_  # 聚类中心
print('各类聚类中心为：\n',kmeans_cc1)
kmeans_labels1 = kmeans_model1.labels_  # 样本的类别标签
print('各样本的类别标签为：\n',kmeans_labels1)
r1 = pd.Series(kmeans_model1.labels_).value_counts()  # 统计不同类别样本的数目
print('最终每个类别的数目为：\n',r1)
# 输出聚类分群的结果
cluster_center1 = pd.DataFrame(kmeans_model1.cluster_centers_,             columns=['Rental Yield','sq_mt_built',
                  'n_rooms','bathroom'])   # 将聚类中心放在数据框中
cluster_center1.index = pd.DataFrame(kmeans_model1.labels_ ).                  drop_duplicates().iloc[:,0]  # 将样本类别作为数据框索引
print(cluster_center1)


# In[116]:


#提前了解雷达图形状
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt 
# 客户分群雷达图
labels = ['Rental Yield','sq_mt_built','n_rooms','bathroom']
legen = ['Room_Type' + str(i + 1) for i in cluster_center1.index]  # 客户群命名，作为雷达图的图例
lstype = ['-','--',(0, (3, 5, 1, 5, 1, 5)),':','-.','solid']
kinds = list(cluster_center1.iloc[:, 0])
# 由于雷达图要保证数据闭合，因此再添加L列，并转换为 np.ndarray
cluster_center1 = pd.concat([cluster_center1, cluster_center1[['Rental Yield']]], axis=1)
centers1 = np.array(cluster_center1.iloc[:, 0:])

# 分割圆周长，并让其闭合
n = len(labels)
angle = np.linspace(0, 2 * np.pi, n, endpoint=False)
angle = np.concatenate((angle, [angle[0]]))

# 绘图
fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, polar=True)  # 以极坐标的形式绘制图形
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号 
# 画线
for i in range(len(kinds)):
    ax.plot(angle, centers1[i], linestyle=lstype[i], linewidth=2, label=kinds[i])
# 添加属性标签
ax.set_thetagrids(angle * 180 / np.pi, labels)
plt.title('Room_Clusters1')
#plt.legend(legen)
#plt.subplots(figsize=(10, 10)) # 设置画面大小
plt.savefig(fname="Room_Type2.png",figsize=[10,10])
plt.show()
plt.close


# In[80]:


#提前了解雷达图形状
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt 
# 客户分群雷达图
labels = ['ROI','parking','lift','SR','pool']
legen = ['Room_Type' + str(i + 1) for i in cluster_center.index]  # 客户群命名，作为雷达图的图例
lstype = ['-','--',(0, (3, 5, 1, 5, 1, 5)),':','-.','solid']
kinds = list(cluster_center.iloc[:, 0])
# 由于雷达图要保证数据闭合，因此再添加L列，并转换为 np.ndarray
cluster_center = pd.concat([cluster_center, cluster_center[['ROI']]], axis=1)
centers = np.array(cluster_center.iloc[:, 0:])

# 分割圆周长，并让其闭合
n = len(labels)
angle = np.linspace(0, 2 * np.pi, n, endpoint=False)
angle = np.concatenate((angle, [angle[0]]))

# 绘图
fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, polar=True)  # 以极坐标的形式绘制图形
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号 
# 画线
for i in range(len(kinds)):
    ax.plot(angle, centers[i], linestyle=lstype[i-1], linewidth=2, label=kinds[i])
# 添加属性标签
ax.set_thetagrids(angle * 180 / np.pi, labels)
plt.title('Room_Clusters')
#plt.legend(legen)
#plt.subplots(figsize=(10, 10)) # 设置画面大小
plt.savefig(fname="Room_Type1.png",figsize=[10,10])
plt.show()
plt.close


# In[54]:


import pandas as pd
import numpy as np
from sklearn.cluster import KMeans  # 导入kmeans算法

# 读取标准化后的数据
BANK_scale = np.load('G:\\BA资料\\DM\\GA\\TFMDR_scale.npz')['arr_0']
k = 5  # 确定聚类中心数

# 构建模型，随机种子设为123
kmeans_model = KMeans(n_clusters=k,n_jobs=4,random_state=123)
fit_kmeans = kmeans_model.fit(BANK_scale)  # 模型训练

# 查看聚类结果
kmeans_cc = kmeans_model.cluster_centers_  # 聚类中心
print('各类聚类中心为：\n',kmeans_cc)
kmeans_labels = kmeans_model.labels_  # 样本的类别标签
print('各样本的类别标签为：\n',kmeans_labels)
r1 = pd.Series(kmeans_model.labels_).value_counts()  # 统计不同类别样本的数目
print('最终每个类别的数目为：\n',r1)
# 输出聚类分群的结果
cluster_center = pd.DataFrame(kmeans_model.cluster_centers_,             columns=['ZA','ZM','ZE','ZL','ZY','ZP'])   # 将聚类中心放在数据框中
cluster_center.index = pd.DataFrame(kmeans_model.labels_ ).                  drop_duplicates().iloc[:,0]  # 将样本类别作为数据框索引
print(cluster_center)


# In[16]:


#提前了解雷达图形状
get_ipython().run_line_magic('matplotlib', 'inline')
import matplotlib.pyplot as plt 
# 客户分群雷达图
labels = ['ZA','ZM','ZE','ZL','ZY','ZP']
legen = ['Customer_Type' + str(i + 1) for i in cluster_center.index]  # 客户群命名，作为雷达图的图例
lstype = ['-','--',(0, (3, 5, 1, 5, 1, 5)),':','-.']
kinds = list(cluster_center.iloc[:, 0])
# 由于雷达图要保证数据闭合，因此再添加L列，并转换为 np.ndarray
cluster_center = pd.concat([cluster_center, cluster_center[['ZA']]], axis=1)
centers = np.array(cluster_center.iloc[:, 0:])

# 分割圆周长，并让其闭合
n = len(labels)
angle = np.linspace(0, 2 * np.pi, n, endpoint=False)
angle = np.concatenate((angle, [angle[0]]))

# 绘图
fig = plt.figure(figsize=(8,6))
ax = fig.add_subplot(111, polar=True)  # 以极坐标的形式绘制图形
plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号 
# 画线
for i in range(len(kinds)):
    ax.plot(angle, centers[i], linestyle=lstype[i], linewidth=2, label=kinds[i])
# 添加属性标签
ax.set_thetagrids(angle * 180 / np.pi, labels)
plt.title('Customer_Features')
#plt.legend(legen)
#plt.subplots(figsize=(10, 10)) # 设置画面大小
plt.savefig(fname="Cus_Type.png",figsize=[10,10])
plt.show()
plt.close


# In[23]:


import numpy as np
import operator
 
 


# In[37]:


# implement knn from scratch
import numpy as np
from collections import Counter

def euclidean_distance(x1,x2):
    distance = np.sqrt(np.sum((x1-x2)**2))
    return distance


# In[34]:


'''class KNN:
    def __init__(self, k=3):
    self.k = k

    def fit(self, X, y):
        self.X_train = X
        self.y_train = y

    def predict(self, X):
        predictions = [self._predict(x) for x in X]
        return predictions

    def _predict(self, x):
        # compute the distance
        distances = [euclidean_distance(x, x_train) for x_train in self.X_train]


        # get the closest k
        k_indices = np.argsort(distances)[:self.k]
        k_nearest_labels = [self.y_train[i] for i in k_indices]

        # majority vote
        most_common = Counter(k_nearest_labels).most_common()
        return most_common[0][0]
'''


# In[36]:


class KNN:
